# -*- coding: utf-8 -*-
"""Progetto - Riconoscimento di fiori per un’azienda AgriTech.ipynb.bak

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eFGiqdDKZ3M2Y0iFccWifHLr7lGGCpoe

#Introduzione

GreenTech Solutions Ltd., un'azienda all'avanguardia nel settore dell'agricoltura tecnologica, si trova ad affrontare la necessità critica di implementare un sistema avanzato di riconoscimento automatico dei fiori all'interno delle sue operazioni quotidiane. La necessità di identificare automaticamente i fiori è cruciale per diversi motivi strategici e operativi:

1. **Ottimizzazione delle Procedure Agricole**: Il riconoscimento automatico dei fiori consente di automatizzare e accelerare processi chiave all'interno dell'agricoltura, come la valutazione della salute delle piante, la gestione dei raccolti e la programmazione delle attività di manutenzione.
2. **Monitoraggio della Salute Vegetale**: La capacità di identificare rapidamente e con precisione i fiori permette a GreenTech Solutions di monitorare la salute delle colture in tempo reale, individuando precocemente eventuali segnali di stress vegetativo, malattie o carenze nutrienti.
3. **Decisioni Basate sui Dati**: Il riconoscimento automatico dei fiori fornisce dati dettagliati e affidabili che supportano decisioni agronomiche informate, come l'ottimizzazione dei trattamenti fitosanitari, la gestione dell'irrigazione e l'allocazione delle risorse agricole.
4. **Efficienza Operativa e Riduzione dei Costi**: Automatizzare il processo di identificazione dei fiori riduce la dipendenza da risorse umane per attività ripetitive e migliorabili, consentendo a GreenTech Solutions di allocare le risorse in modo più efficiente e di ridurre i costi operativi complessivi.

**Benefici del Progetto**: L'implementazione di un modello avanzato di Computer Vision per il riconoscimento automatico dei fiori porterà numerosi vantaggi tangibili:

 - Aumento della Produttività: Riduzione dei tempi dedicati all'identificazione manuale dei fiori, liberando risorse per attività più strategiche e ad alto valore aggiunto.
 - Miglioramento della Qualità: Garanzia di una classificazione più precisa e uniforme dei fiori, migliorando la qualità complessiva dei servizi e dei prodotti agricoli offerti.
 - Innovazione Tecnologica: Dimostrazione di leadership nell'adozione di
tecnologie all'avanguardia, posizionando GreenTech Solutions come pioniere nell'agricoltura digitale e sostenibile.

**Dettagli del Progetto**: GreenTech Solutions Ltd. ha avviato un'iniziativa strategica per sviluppare un prototipo di modello di Computer Vision basato su intelligenza artificiale per il riconoscimento automatico dei fiori. Il dataset fornito include due categorie principali di fiori:

- Daisy (Margherita): 529 immagini di train, 163 di validation, 77 di test.
- Dandelion (Tarassaco): 746 immagini di train, 201 di validation, 105 di test.

**Obiettivo del Progetto**: Il principale obiettivo è sviluppare un modello altamente accurato e robusto, capace di classificare automaticamente i fiori con il miglior F1-score (macro) possibile sul dataset di test. L'uso di tecniche avanzate come il transfer learning e le data augmentations con la libreria timm di PyTorch è fortemente consigliato per ottimizzare le prestazioni del modello.

**Approccio Raccomandato**: Si richiede la configurazione e l'addestramento di una rete neurale profonda utilizzando PyTorch, con una documentazione dettagliata delle metodologie adottate e delle decisioni prese durante il processo di sviluppo. È essenziale valutare attentamente i vantaggi e le sfide del modello, inclusa la sua capacità di adattarsi a variazioni nelle condizioni ambientali e di produrre risultati coerenti e affidabili in scenari reali.

L'integrazione di questa soluzione innovativa permetterà a GreenTech Solutions Ltd. di consolidare ulteriormente la sua posizione nel mercato agricolo, offrendo soluzioni avanzate che migliorano l'efficienza operativa, promuovono la sostenibilità e soddisfano le crescenti esigenze dei clienti nel settore agricolo moderno.

**Dataset**

Link: https://proai-datasets.s3.eu-west-3.amazonaws.com/progetto-finale-flowes.tar.gz

#Progetto

##Import Librerie
"""

# Standard library
import os
import tarfile
import urllib.request
import time
import random
from collections import Counter
import warnings

# Third-party libraries
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import torchvision
import torchvision.utils as vutils
from torchvision import datasets, transforms, models
import timm
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score

# Suppress warnings
warnings.filterwarnings("ignore")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""##Funzioni

###Download ed Estrazione Dataset
"""

def download_and_extract(dataset_url, dataset_tar, dataset_path):
    if not os.path.exists(dataset_tar):
        print("Scaricamento del dataset...")
        urllib.request.urlretrieve(dataset_url, dataset_tar)

    if not os.path.exists(dataset_path) or len(os.listdir(dataset_path)) == 0:
        print("Estrazione del dataset...")
        with tarfile.open(dataset_tar, "r:gz") as tar:
            tar.extractall()
        print("Dataset estratto con successo!")
    else:
        print("Dataset già presente e integro.")

"""###Preparazione e Caricamento Dati"""

# Trasformazioni per il preprocessing e la data augmentation
def compute_dataset_mean_std(loader):
    mean = torch.zeros(3)
    std = torch.zeros(3)
    total_images = 0

    for images, _ in loader:
        if not isinstance(images, torch.Tensor):
            images = transforms.ToTensor()(images)

        batch_samples = images.size(0)
        images = images.view(batch_samples, 3, -1)
        mean += images.mean(dim=2).sum(dim=0)
        std += images.std(dim=2).sum(dim=0)
        total_images += batch_samples

    mean /= total_images
    std /= total_images

    return mean.tolist(), std.tolist()

def is_valid_file(path):
    filename = os.path.basename(path)
    return not filename.startswith(".")

# Funzione per caricare il dataset
def load_data(data_dir, batch_size=32):
    train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=train_transforms, is_valid_file=is_valid_file)
    val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'valid'), transform=val_test_transforms, is_valid_file=is_valid_file)
    test_dataset = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=val_test_transforms, is_valid_file=is_valid_file)

    dataset_mean, dataset_std = compute_dataset_mean_std(DataLoader(train_dataset, batch_size=batch_size))
    print(f"Dataset Mean: {dataset_mean}, Dataset Std: {dataset_std}")

    train_transforms.transforms.append(transforms.Normalize(mean=dataset_mean, std=dataset_std))
    val_test_transforms.transforms.append(transforms.Normalize(mean=dataset_mean, std=dataset_std))

    print("Numero di immagini nel training set:", len(train_dataset))
    print("Numero di immagini nel validation set:", len(val_dataset))
    print("Numero di immagini nel test set:", len(test_dataset))

    num_workers = 4

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

    return train_loader, val_loader, test_loader, train_dataset.class_to_idx

"""###Visualizzazione Dati"""

# Visualizzazione immagini dataset
def show_sample_images(loader, class_names):
    data_iter = iter(loader)
    images, labels = next(data_iter)
    images = images[:8]
    labels = labels[:8]

    fig, ax = plt.subplots(figsize=(12, 6))
    grid = vutils.make_grid(images, nrow=4, normalize=True)
    ax.imshow(grid.permute(1, 2, 0))

    class_labels = [class_names.get(label, "Unknown") for label in labels.tolist()]
    ax.set_title(", ".join(class_labels))
    ax.axis("off")
    plt.show()

def plot_class_distribution(dataset):
    labels = [label for _, label in dataset.samples]
    class_counts = Counter(labels)
    sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))

"""###Data Augmentation"""

def mixup_data(x, y, alpha=0.4):
    """Applica Mixup alle immagini e alle etichette"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def cutmix_data(x, y, alpha=1.0):
    """Applica CutMix alle immagini e alle etichette"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)

    bbx1, bby1, bbx2, bby2 = 32, 32, 192, 192
    mixed_x = x.clone()
    mixed_x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]
    y_a, y_b = y, y[index]

    return mixed_x, y_a, y_b, lam

def criterion_with_augmentation(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

"""###Creazione Modello e Funzioni Inference"""

def create_model(model_name, num_classes=2, pretrained=True):
    model = timm.create_model(model_name, pretrained=pretrained)

    if "resnet" in model_name:
        in_features = model.fc.in_features
        model.fc = nn.Linear(in_features, num_classes)
    else:
        in_features = model.get_classifier().in_features
        model.classifier = nn.Linear(in_features, num_classes)

    return model

# Test prestazioni batch di immagini
def test_inference_speed(model, device, batch_size=32, image_size=(3, 224, 224), num_runs=10):
    model.to(device)
    model.eval()
    dummy_input = torch.randn(batch_size, *image_size).to(device)

    with torch.no_grad():
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        start_time = time.time()
        for _ in range(num_runs):
            _ = model(dummy_input)
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        elapsed_time = (time.time() - start_time) / num_runs

    return elapsed_time

def predict_sample_image(model, dataloader, device, class_names):
    model.eval()
    model.to(device)

    images, labels = next(iter(dataloader))
    images = images.to(device)

    with torch.no_grad():
        outputs = model(images)
        _, preds = torch.max(outputs, 1)

    fig, axes = plt.subplots(1, 5, figsize=(15, 4))
    for idx in range(5):
        ax = axes[idx]
        img = images[idx].cpu().permute(1, 2, 0).numpy()
        img = img * 0.225 + 0.45
        img = img.clip(0, 1)
        ax.imshow(img)
        ax.set_title(f"Pred: {class_names[preds[idx].item()]}")
        ax.axis("off")

    plt.show()

"""###Addestramento Modello"""

def train_model(model, train_loader, val_loader, device, epochs=10, lr=1e-4, patience=3):
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    weight_decay = 1e-2
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    scaler = torch.cuda.amp.GradScaler()
    writer = SummaryWriter()

    if isinstance(model, models.ResNet):
        lr = 3e-5
        weight_decay = 1e-1
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)
    else:
        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)

    model.to(device)
    best_f1 = 0.0
    patience_counter = 0

    train_losses, val_losses = [], []
    train_accs, val_accs = [], []

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()

            with torch.amp.autocast(device_type="cuda"):
                outputs = model(inputs)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            running_loss += loss.item() * inputs.size(0)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        train_loss = running_loss / total
        train_acc = correct / total
        train_losses.append(train_loss)
        train_accs.append(train_acc)

        # Validation
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        all_preds = []
        all_labels = []

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                val_loss += loss.item() * inputs.size(0)
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        val_f1 = f1_score(all_labels, all_preds, average='macro')
        val_losses.append(val_loss / total)
        val_accs.append(correct / total)

        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - Val Loss: {val_loss:.4f}, Val Acc: {correct/total:.4f}, Val F1: {val_f1:.4f}")

        writer.add_scalar('F1/val', val_f1, epoch)
        scheduler.step() if not isinstance(model, models.ResNet) else scheduler.step(val_f1)

        if val_f1 > best_f1:
            best_f1 = val_f1
            patience_counter = 0
            torch.save(model.state_dict(), f"best_{model.__class__.__name__}.pth")
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print("Early stopping triggered.")
            break

    writer.close()
    return train_losses, val_losses, train_accs, val_accs

def plot_training_curves(model, train_losses, val_losses, train_accs, val_accs):
    """Visualizza le curve di loss e accuracy per l'addestramento del modello specificato."""
    model_name = model.__class__.__name__

    plt.figure(figsize=(12, 5))

    # Loss Plot
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label="Train Loss")
    plt.plot(val_losses, label="Val Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title(f"Loss Curve - {model_name}")
    plt.legend()

    # Accuracy Plot
    plt.subplot(1, 2, 2)
    plt.plot(train_accs, label="Train Acc")
    plt.plot(val_accs, label="Val Acc")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.title(f"Accuracy Curve - {model_name}")
    plt.legend()

    plt.show()

"""###Valutazione Modello"""

def evaluate_model(model, test_loader, device, class_names):
    model.to(device)
    model.eval()

    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)

    model_name = model.default_cfg['architecture'] if hasattr(model, 'default_cfg') else model.__class__.__name__
    print(f"Modello: {model_name}")
    print(f"Accuratezza sul test set: {accuracy:.4f}")
    print("Valori unici delle predizioni:", set(all_preds))
    print("Valori unici delle etichette reali:", set(all_labels))

    report = classification_report(all_labels, all_preds, target_names=class_names)
    print(report)

    # Confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

def plot_misclassified_images(model, test_loader, device, class_names, num_images=5):
    model.to(device)
    model.eval()

    misclassified_images = []
    misclassified_labels = []
    misclassified_preds = []

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, preds = torch.max(outputs, 1)

            misclassified_indices = (preds != labels).cpu().numpy().nonzero()[0]
            for idx in misclassified_indices:
                misclassified_images.append(images[idx].cpu())
                misclassified_labels.append(labels[idx].item())
                misclassified_preds.append(preds[idx].item())

    if len(misclassified_images) == 0:
        print("Nessuna immagine misclassificata!")
        return

    num_images = min(num_images, len(misclassified_images))
    selected_indices = random.sample(range(len(misclassified_images)), num_images)

    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))
    for i, idx in enumerate(selected_indices):
        image = misclassified_images[idx].permute(1, 2, 0).numpy()
        true_label = class_names[misclassified_labels[idx]]
        predicted_label = class_names[misclassified_preds[idx]]

        axes[i].imshow(image)
        axes[i].set_title(f"Pred: {predicted_label}\nTrue: {true_label}", color='red')
        axes[i].axis("off")

    plt.show()

"""##Dataset"""

# Scaricare ed estrarre il dataset
dataset_url = "https://proai-datasets.s3.eu-west-3.amazonaws.com/progetto-finale-flowes.tar.gz"
dataset_tar = "dataset.tar.gz"
dataset_path = "progetto-finale-flowes"

train_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

val_test_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

download_and_extract(dataset_url, dataset_tar, dataset_path)
train_loader, val_loader, test_loader, class_to_idx = load_data(dataset_path)
show_sample_images(train_loader, {v: k for k, v in class_to_idx.items()})

"""##Test Modelli"""

resnet_model = create_model("resnet50", num_classes=len(class_to_idx))
efficientnet_model = create_model("efficientnet_b0", num_classes=len(class_to_idx))
vit_model = create_model("vit_base_patch16_224", num_classes=len(class_to_idx))

resnet_time = test_inference_speed(resnet_model, device)
efficientnet_time = test_inference_speed(efficientnet_model, device)
vit_time = test_inference_speed(vit_model, device)

print(f"Tempo di inferenza ResNet50: {resnet_time:.4f} sec")
print(f"Tempo di inferenza EfficientNet: {efficientnet_time:.4f} sec")
print(f"Tempo di inferenza ViT: {vit_time:.4f} sec")

"""##Addestramento Modelli

###EfficentNet
"""

train_losses, val_losses, train_accs, val_accs = train_model(efficientnet_model, train_loader, val_loader, device, epochs=15, lr=5e-5, patience=5)
predict_sample_image(efficientnet_model, test_loader, device, {v: k for k, v in class_to_idx.items()})

plot_training_curves(efficientnet_model, train_losses, val_losses, train_accs, val_accs)

"""1. **Andamento della Loss e Accuracy**
 - La train loss mostra un comportamento irregolare senza un chiaro trend di decrescita. Si notano oscillazioni significative, il che suggerisce che il modello non stia convergendo in modo stabile.
 - L'accuratezza di training oscilla tra il 69% e l'80%, senza un chiaro miglioramento progressivo.

2. **Performance sulla Validation**
 - La val loss rimane costantemente molto alta (~96-106), il che è sospetto. Probabilmente è dovuto a una scala diversa rispetto alla train loss o a un problema nella funzione di perdita.
 - L'accuratezza e l'F1-score sulla validazione sono molto alti fin dalle prime epoche (97-98%), indicando un modello che generalizza bene nonostante l'instabilità della train loss.

3. **Segnali di Overfitting o Underfitting**
 - Il fatto che la train loss rimanga alta mentre la validation accuracy è eccellente suggerisce un possibile problema di mismatch tra i due set di dati o un'anomalia nel calcolo della loss.
 - Se il dataset di validazione è troppo piccolo o troppo simile a quello di training, il modello potrebbe semplicemente memorizzarlo.
 - Non si vede un chiaro overfitting perché la validazione rimane costantemente buona, ma la mancata diminuzione della train loss è sospetta.

4. **Early Stopping**
 - È stato attivato l'early stopping dopo la 12ª epoca, segnalando che il modello non stava migliorando ulteriormente.
 - L'accuratezza era già molto alta, quindi fermarsi qui è ragionevole.

5. **Possibili Miglioramenti**

 - Analizzare la funzione di perdita: il valore della val loss sembra anomalo rispetto all'accuratezza, potrebbe esserci un problema nel suo calcolo.
 - Aumentare il batch size o usare gradient clipping per ridurre la variabilità tra le epoche.
 - Verificare il dataset per assicurarsi che non ci sia leakage tra training e validation.
 - Testare learning rate diversi o usare un scheduler più aggressivo per evitare oscillazioni.

###ResNet-50
"""

train_losses, val_losses, train_accs, val_accs = train_model(resnet_model, train_loader, val_loader, device, epochs=15, lr=5e-5, patience=5)
predict_sample_image(resnet_model, test_loader, device, {v: k for k, v in class_to_idx.items()})

plot_training_curves(resnet_model, train_losses, val_losses, train_accs, val_accs)

"""1. **Andamento della Loss e Accuracy**

- La train loss non mostra un trend chiaro di discesa: oscilla tra 0.45 e 0.52, suggerendo una convergenza instabile.
- L'accuratezza di training è relativamente bassa (66-70%) e non migliora in modo significativo, il che può indicare difficoltà nell'apprendimento o la necessità di un learning rate diverso.

2. **Performance sulla Validation**

- La validation loss parte da un valore molto alto (~ 124) e scende leggermente (~ 114), ma rimane elevata. Questo può suggerire uno squilibrio nei valori di loss tra training e validation.
- L'accuratezza e l'F1-score sulla validazione sono molto buoni (~94-95%) già dalle prime epoche, il che fa pensare a un problema di scala nella loss piuttosto che a una vera difficoltà del modello.

3. **Segnali di Overfitting o Underfitting**

- La train loss che non scende e la validation accuracy costantemente alta suggeriscono che il modello potrebbe non star apprendendo in modo efficace dai dati di training.
- Questo potrebbe indicare underfitting, ossia il modello non riesce ad adattarsi bene ai dati di training.
- Tuttavia, l'accuratezza di validazione alta suggerisce che il modello ha comunque una buona capacità di generalizzazione.

4. **Early Stopping**

- Il training si interrompe all'epoca 9 su 15, segnalando che il modello ha smesso di migliorare.
- Dato che la validation accuracy era già alta, fermarsi qui è ragionevole, ma sarebbe utile indagare sul motivo per cui la train loss non è mai scesa in modo significativo.

5. **Possibili Miglioramenti**

- Analizzare la funzione di perdita: la validation loss è molto alta rispetto all'accuracy, quindi potrebbe esserci un problema di scala.
- Regolare il learning rate: provare un valore più basso o usare un scheduler più aggressivo.
- Aumentare il numero di epoche o cambiare strategia di early stopping, dato che la rete potrebbe aver bisogno di più tempo per convergere.
- Esplorare diverse tecniche di regularization, come dropout o data augmentation più avanzata, per verificare se l'underfitting è dovuto a una mancanza di capacità espressiva del modello.

###ViT
"""

train_losses, val_losses, train_accs, val_accs = train_model(vit_model, train_loader, val_loader, device, epochs=15, lr=5e-5, patience=5)
predict_sample_image(vit_model, test_loader, device, {v: k for k, v in class_to_idx.items()})

plot_training_curves(vit_model, train_losses, val_losses, train_accs, val_accs)

"""1. **Andamento della Loss e dell'Accuracy**
- Train Loss: inizia molto alta (2.54) e scende gradualmente fino a circa 1.30-1.35.
- Train Accuracy: parte da un valore basso (49.96%) ma cresce rapidamente, raggiungendo circa il 75%. Tuttavia, non sembra migliorare molto dopo le prime epoche.
- Validation Loss: comincia estremamente alta (~ 512) e diminuisce nel tempo (~ 401), ma rimane molto elevata rispetto ai valori di loss tipici delle CNN. Questo potrebbe essere dovuto alla scala della loss o a un problema di convergenza.

2. **Performance sulla Validation**
- Validation Accuracy migliora rapidamente da 88% a oltre 97% nelle prime 5 epoche e continua a oscillare attorno a questo valore.
- Validation F1-score segue lo stesso andamento, mostrando buone prestazioni già dopo poche epoche.
- La loss di validazione rimane elevata, il che potrebbe suggerire che la funzione di perdita stia generando valori su una scala diversa rispetto a quella delle CNN.

3. **Segnali di Overfitting o Underfitting**
- Il modello raggiunge una validation accuracy molto alta, quindi non sembra soffrire di overfitting.
- Tuttavia, la train loss è molto alta rispetto alle CNN (come EfficientNet e ResNet), il che potrebbe indicare che il modello sta facendo fatica a ridurre l’errore complessivo.
- Potrebbe esserci un problema di underfitting relativo, dove il modello non sta sfruttando appieno la capacità di apprendimento.

4. **Early Stopping**
- Il modello si ferma alla 13ª epoca su 15, il che suggerisce che il miglioramento era minimo nelle ultime iterazioni.
- Dato che l'accuracy è elevata, fermarsi è ragionevole, ma rimane il problema della loss elevata.

5. **Possibili Miglioramenti**
- Verificare la funzione di perdita: potrebbe essere utile eseguire un'analisi della scala dei valori di loss rispetto agli altri modelli.
- Aumentare il numero di epoche: potrebbe essere utile testare una convergenza più lunga per vedere se la loss continua a scendere.
- Regolare il learning rate: potrebbe essere necessario un valore più basso o una regolazione più graduale con scheduler.
- Aggiungere un meccanismo di regularizzazione (dropout, weight decay) per verificare se può stabilizzare meglio l’addestramento.
- Confrontare i risultati finali con EfficientNet e ResNet, specialmente sulla capacità di generalizzazione.

##Valutazione Modelli

###EfficientNet-B0
"""

evaluate_model(efficientnet_model, test_loader, device, list(class_to_idx.keys()))
plot_misclassified_images(efficientnet_model, test_loader, device, list(class_to_idx.keys()))

"""I risultati mostrano che il modello ha ottenuto prestazioni eccellenti:

- **Accuratezza**: Il **97.8%** sul test set indica una forte capacità del modello di distinguere tra le classi, un risultato eccellente, soprattutto considerando che si tratta di un modello relativamente leggero come EfficientNet-B0. Questo suggerisce che il modello ha generalizzato bene senza overfitting significativo.
- **Metriche di classificazione**: ***Precision, Recall*** e ***F1-Score*** sono molto elevate per entrambe le classi (*daisy* e *dandelion*), con valori compresi tra **0.96** e **0.99**.
Queste metriche suggeriscono che il modello ha appreso a riconoscere correttamente le caratteristiche distintive di ciascuna classe, con pochissimi falsi positivi o falsi negativi.
- **Matrice di confusione**: La matrice mostra che per la classe daisy il modello ha classificato correttamente 74 immagini e ne ha sbagliate 3, mentre per dandelion ha classificato correttamente 104 immagini commettendo solo 1 errore.
Questo conferma l'elevata precisione nella distinzione delle due classi.
- **Considerazioni finali**

  Il modello è molto accurato e bilanciato, con un'ottima capacità di riconoscimento delle due classi.
  I falsi positivi e falsi negativi sono pochi e distribuiti in modo accettabile, quindi non ci sono problemi evidenti di bias.
  Per migliorare ulteriormente, si potrebbero esplorare tecniche come,
  Fine-tuning più profondo, ulteriori augmentations mirate, esperimenti con modelli più complessi.

In sintesi, il modello si comporta in modo ottimale nel classificare le immagini, e con un piccolo aggiustamento nella fase di visualizzazione potrai anche migliorare la chiarezza dei plot. Ma nel complesso, i risultati sono eccellenti e pronti per un'applicazione reale.

###ResNet-50
"""

evaluate_model(resnet_model, test_loader, device, list(class_to_idx.keys()))
plot_misclassified_images(resnet_model, test_loader, device, list(class_to_idx.keys()))

"""I risultati del modello mostrano performance molto solide, sebbene leggermente inferiori rispetto al modello EfficientNet_b0:

- **Accuratezza**: Con un'accuratezza del **95.6%**, il modello classifica correttamente la maggior parte delle immagini del test set.
- **Metriche di classificazione**:
 - Per la classe **daisy** il modello raggiunge una ***precision*** del **94%**, ***recall*** del **96%** e un ***F1-score*** del **95%**.
 - Per la classe **dandelion** i valori sono ancora migliori, con **precision** al **97%**, ***recall*** al **95%** e ***F1-score*** al **96%**.
 - Le **medie** (macro e weighted) si attestano intorno al **95-96%**, il che indica una buona consistenza tra le classi.
- **Matrice di confusione**:
La matrice mostra 74 corretti e 3 errori per la classe **daisy**; mentre per la classe **dandelion** ci sono 100 corretti e 5 errori.
Questi errori sono minimi e indicano che il modello, pur essendo molto performante, potrebbe beneficiare di ulteriori affinamenti (ad esempio, tramite tuning di iperparametri o tecniche di data augmentation) per ridurre ulteriormente gli errori.
- **Considerazioni finali**

  I risultati indicano che ResNet50 ha appreso in maniera solida a distinguere tra "daisy" e "dandelion", con performance molto vicine alle migliori soluzioni, pur evidenziando una lieve difficoltà nel distinguere alcune immagini di "daisy" rispetto a "dandelion". Questi risultati sono molto promettenti, soprattutto considerando la complessità intrinseca del task di classificazione tra le due classi.
  
  Se l'obiettivo è raggiungere performance ancora più elevate, si potrebbe esplorare l'uso di tecniche di data augmentation aggiuntive, ottimizzare ulteriormente l'iperparametrizzazione o sperimentare con architetture differenti.

In sintesi, ***ResNet50*** dimostra un'elevata capacità di classificazione, sebbene l'approccio con ***EfficientNet*** risulti leggermente superiore in termini di accuratezza e bilanciamento tra precisione e recall. La scelta tra i due modelli potrebbe dunque dipendere da altri fattori come la velocità di inferenza.

###ViT
"""

evaluate_model(vit_model, test_loader, device, list(class_to_idx.keys()))
plot_misclassified_images(vit_model, test_loader, device, list(class_to_idx.keys()))

"""I risultati del modello mostrano prestazioni molto competitive e alcune osservazioni interessanti:

- **Accuratezza**: L'**accuratezza** del **97.25%** sul test set evidenzia che il modello è in grado di classificare correttamente la maggior parte delle immagini. Questa elevata accuratezza indica una buona capacità di generalizzazione, nonostante la natura complessa dei dati.
- **Metriche di classificazione**:
  - Per la classe ***daisy***, precisione e recall sono rispettivamente del **97%** e **96%**, con un ***F1-score*** del **97%**.
  - Per la classe ***dandelion***, i valori sono ancora migliori, con ***precision*** al **97%**, ***recall*** al **98%** e **`*F1-score*`** al **98%**.
  - Queste metriche indicano un equilibrio ottimale tra i falsi positivi e falsi negativi, confermando la robustezza del modello.
- **Matrice di confusione**:La matrice mostra che per ***daisy*** sono state correttamente classificate 74 immagini con 3 errori, mentre per ***dandelion*** 103 immagini sono state classificate correttamente con solo 2 errori.
Questo conferma che gli errori sono minimi e il modello gestisce bene entrambe le classi. Notiamo una leggera asimmetria, il modello sembra avere una leggera tendenza a confondere alcuni casi di "daisy" (3 errori) rispetto a "dandelion" (2 errori), anche se entrambi i valori sono molto bassi.

- **Considerazioni finali**

  Il modello ViT, basato su architettura di transformer, dimostra di saper cogliere caratteristiche rilevanti nelle immagini. Le prestazioni evidenti in termini di precisione, recall e F1-score suggeriscono che la segmentazione in patch e l'attenzione globale stanno funzionando bene per questo task.
  Eventuali errori residui potrebbero derivare da situazioni in cui le caratteristiche visive delle classi si sovrappongono parzialmente o da immagini particolarmente difficili da interpretare, ma in generale il modello è estremamente affidabile.

In conclusione, ViT si comporta in modo eccellente, offrendo un bilanciamento molto buono tra precisione e recall, e rappresenta una valida alternativa ai modelli CNN tradizionali, dimostrando la potenzialità degli approcci basati su transformer anche nel campo della visione artificiale.

##Conclusioni e Analisi dei Risultati

L'obiettivo del progetto era sviluppare un modello di deep learning in grado di classificare accuratamente le immagini di fiori in due categorie: Daisy e Dandelion. Sono stati testati tre modelli principali: **ResNet-50**, **EfficientNet-B0** e **ViT(Vision Transformer)**. Di seguito vengono analizzati i risultati ottenuti e le considerazioni finali.

1. **EfficientNet-B0**:
 - Ha ottenuto la miglior **accuratezza** sul test set (**97.80%**), risultando il *modello più performante* tra quelli testati.
 - Il bilanciamento tra **precision** e **recall** è ottimale, con un **F1-score** medio di **0.98**.
 - La matrice di confusione mostra pochi errori, con solo 4 immagini classificate erroneamente.
 - L'inferenza è stata la più rapida tra i modelli testati (0.0456 sec), rendendolo la scelta ideale per applicazioni in tempo reale.

2. **ResNet-50**:
 - Ha ottenuto un'**accuratezza** del **95.60%**, inferiore rispetto a *EfficientNet-B0*.
 - Il modello ha mostrato una leggera tendenza a confondere alcune immagini, in particolare tra Dandelion e Daisy, come evidenziato dalla matrice di confusione.
 - Il tempo di inferenza è stato di 0.1614 sec, più elevato rispetto a EfficientNet-B0, ma comunque accettabile per molte applicazioni.

3. **ViT (Vision Transformer)**:
 - Ha raggiunto un'**accuratezza** del **97.25%**, avvicinandosi alle prestazioni di *EfficientNet-B0*.
 - La **precision** e il **recall** sono molto equilibrati, con un **F1-score** medio di **0.97**.
 - Tuttavia, il tempo di inferenza è stato il più alto (0.3291 sec), rendendolo meno adatto a scenari che richiedono latenza ridotta.

####**Considerazioni Finali**

- ***Performance vs Complessità***: **EfficientNet-B0** ha dimostrato di essere il modello più efficace considerando il compromesso tra accuratezza e tempo di inferenza. **ResNet-50**, pur avendo buone performance, è stato superato in termini di precisione e velocità. **ViT** ha fornito buoni risultati, ma il costo computazionale lo rende meno vantaggioso per applicazioni in tempo reale.
- ***Pre-processing e Ottimizzazione***: L'accuratezza elevata suggerisce che il pre-processing delle immagini è adeguato, ma potrebbero essere esplorate ulteriori tecniche di data augmentation per migliorare la robustezza del modello.
- ***Sviluppi futuri***: Per migliorare ulteriormente la classificazione, si potrebbe sperimentare con architetture più avanzate di **EfficientNet**, l'uso di ensemble learning o l'applicazione di tecniche di pruning e quantizzazione per ridurre il costo computazionale mantenendo alte prestazioni.

In conclusione, **EfficientNet-B0** si conferma la scelta ottimale per il problema affrontato, bilanciando precisione e velocità. Tuttavia, l'esplorazione di modelli più avanzati e ottimizzazioni mirate potrebbe portare ulteriori miglioramenti nel futuro.
"""